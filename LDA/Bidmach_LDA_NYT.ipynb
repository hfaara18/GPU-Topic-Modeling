{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from __future__ import division, print_function\n",
    "import lda\n",
    "os.chdir(\"E:\\\\UVA\\\\Capstone\\\\DataSet\\\\UCI\")\n",
    "from scipy import linalg,dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_matrix(data_file,vocab_file,max_doc,word_sparsity,word_ubiquity):\n",
    "    word_dict = {}\n",
    "    vocab_dict = {}\n",
    "    vocab_list = []\n",
    "    doc = \"\"\n",
    "    last_doc_id = 1\n",
    "    doc_dict = {}\n",
    "    col_index = 0\n",
    "    \n",
    "    #Extract from Vocab file\n",
    "    \n",
    "    vocab_file.seek(0)\n",
    "    line = vocab_file.readline()\n",
    "    \n",
    "    #Create Word vector along with its position index\n",
    "    while line:\n",
    "        vocab_list.append(line.strip())\n",
    "        vocab_dict[line.strip()] = col_index\n",
    "        col_index += 1\n",
    "        line = vocab_file.readline()\n",
    "    \n",
    "    #Extract from data file\n",
    "    \n",
    "    data_file.seek(0)\n",
    "    line = data_file.readline()\n",
    "    doc_vec = [0] * len(vocab_dict)\n",
    "    #doc_vec = np.zeros(len(vocab_dict))\n",
    "    #tdm = np.empty((0,len(vocab_dict)), int)\n",
    "    tdm = []\n",
    "    \n",
    "    count = 0\n",
    "    while line:\n",
    "        col = line.strip().split(' ')\n",
    "        if len(col) == 3:\n",
    "            doc_id = int(col[0])\n",
    "            word_id = int(col[1]) - 1\n",
    "            word_count = int(col[2])\n",
    "                  \n",
    "            doc_vec[word_id] += word_count\n",
    "            \n",
    "            if doc_id != last_doc_id:\n",
    "                #tdm = np.append(tdm,doc_vec, axis=0)\n",
    "                tdm.append(doc_vec)\n",
    "                doc_vec = [0] * len(vocab_dict)\n",
    "            if doc_id==max_doc+1:\n",
    "                break\n",
    "            \n",
    "            last_doc_id = doc_id\n",
    "        line = data_file.readline()\n",
    "    \n",
    "    tdm = np.asarray(tdm)\n",
    "    print(\"Original Matrix: dim = \" + str(tdm.shape))\n",
    "    nrow,ncol = tdm.shape\n",
    "    tdm = tdm[:,tdm.sum(axis=0)>10] #if the word appears in more than 10 docs, then its considered valid\n",
    "    print(\"Deleted all zero columns: dim: \" + str(tdm.shape))\n",
    "    tdm = np.delete(tdm, np.nonzero((tdm==0).sum(axis=0) > nrow*(1-word_sparsity)), axis=1)\n",
    "    print(\"After removing sparse words : dim = \" + str(tdm.shape))\n",
    "    tdm = np.delete(tdm, np.nonzero((tdm==0).sum(axis=0) < nrow*(1-word_ubiquity)), axis=1)\n",
    "    print(\"After removing obiquitous words: dim = \" +str(tdm.shape))\n",
    "    vocab_file.close()\n",
    "    data_file.close()\n",
    "    return tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix: dim = (34982, 102660)\n",
      "Deleted all zero columns: dim: (34982, 46135)\n",
      "After removing sparse words : dim = (34982, 46135)\n",
      "After removing obiquitous words: dim = (34982, 46135)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34982, 46135)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open(\"docword.nytimes.txt\\\\docword.nytimes.txt\", 'r')\n",
    "vocab_file = open(\"vocab.nytimes.txt\", 'r')\n",
    "\n",
    "num_doc = 35000 #Number of documents (Matrix row length)\n",
    "word_sparsity = 0 #0.01 means remove words that appear in less than 1% of documents (sparse words)\n",
    "word_ubiquity = 0.8 #remove words that appear in more than  80% of documents (stopwords)\n",
    "tdm = extract_matrix(data_file,vocab_file,num_doc,word_sparsity,word_ubiquity)\n",
    "tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binfile = open(\"matrix_35000_docs.bin\",\"wb\")\n",
    "np.save(binfile,tdm)\n",
    "binfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (34982, 46135), array([[0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = np.load(\"matrix_35000_docs.bin\")\n",
    "type(mat),mat.shape,mat[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\Savi\\\\Downloads\\\\BIDMach_1.0.3-win-x86_64\\\\mat25000.txt\", mat, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val a = loadFMat(\"mat1000.txt\")\n",
    "val k = 50\n",
    "val (nn,opts) = LDA.learner(a,k) #https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf\n",
    "#val (nn, opts) = LDA.learnPar(a) \n",
    "#val (nn,opts) = LDA.learnBatch(a,k)\n",
    "#val (nn,opts) = LDAgibbs.learner(a,k)\n",
    "opts.what\n",
    "opts.nthreads=number of gpus\n",
    "opts.npasses = 1\n",
    "nn.train\n",
    "nn.modelmats(0) # the topic-word matrix, one topic per row. (looks like topic document matrix)\n",
    "nn.datamats(1) # the factorized topic-document matrix, one document per column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X): <class 'numpy.ndarray'>\n",
      "shape: (34982, 46135)\n",
      "\n",
      "type(vocab): <class 'list'>\n",
      "len(vocab): 102660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document-term matrix\n",
    "X = mat\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# vocab file\n",
    "vocab = [line for line in open(\"E:\\\\UVA\\\\Capstone\\\\DataSet\\\\UCI\\\\vocab.nytimes.txt\")]\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 2min 23s per loop\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=50, n_iter=1, random_state=1)\n",
    "%timeit -n1 -r1 model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(topic_word): <class 'numpy.ndarray'>\n",
      "shape: (20, 7455)\n"
     ]
    }
   ],
   "source": [
    "# topic-word probabilities\n",
    "topic_word = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "print(\"shape: {}\".format(topic_word.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0 sum: 1.000000000000148\n",
      "topic: 1 sum: 0.9999999999999072\n",
      "topic: 2 sum: 0.9999999999999583\n",
      "topic: 3 sum: 0.999999999999847\n",
      "topic: 4 sum: 0.9999999999999427\n"
     ]
    }
   ],
   "source": [
    "#the probabilities of the words should be normalized.\n",
    "for n in range(5):\n",
    "    sum_pr = sum(topic_word[n,:])\n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Topic 0\n",
      "- chastened\n",
      " barefooted\n",
      " colonization\n",
      " blowup\n",
      " combed\n",
      "\n",
      "*Topic 1\n",
      "- aftershock\n",
      " adventurous\n",
      " bask\n",
      " bookshelf\n",
      " bdaley\n",
      "\n",
      "*Topic 2\n",
      "- collapsing\n",
      " commander\n",
      " clarity\n",
      " chimney\n",
      " arranger\n",
      "\n",
      "*Topic 3\n",
      "- chinned\n",
      " abolishing\n",
      " abduction\n",
      " bleaker\n",
      " advantaged\n",
      "\n",
      "*Topic 4\n",
      "- bulgarian\n",
      " carbonated\n",
      " blueberry\n",
      " berries\n",
      " bluefin\n",
      "\n",
      "*Topic 5\n",
      "- comforted\n",
      " blurting\n",
      " cockfighting\n",
      " birdseed\n",
      " backgammon\n",
      "\n",
      "*Topic 6\n",
      "- bitching\n",
      " checkout\n",
      " accelerates\n",
      " classically\n",
      " cetacean\n",
      "\n",
      "*Topic 7\n",
      "- berries\n",
      " agordon\n",
      " choreographing\n",
      " capstone\n",
      " burst\n",
      "\n",
      "*Topic 8\n",
      "- arrayed\n",
      " cheered\n",
      " biomechanic\n",
      " berries\n",
      " charging\n",
      "\n",
      "*Topic 9\n",
      "- backspin\n",
      " billing\n",
      " aspartame\n",
      " calla\n",
      " changeable\n",
      "\n",
      "*Topic 10\n",
      "- allow\n",
      " antler\n",
      " attractively\n",
      " atypically\n",
      " batsman\n",
      "\n",
      "*Topic 11\n",
      "- agencies\n",
      " aftertaste\n",
      " coagulant\n",
      " brian\n",
      " agencias\n",
      "\n",
      "*Topic 12\n",
      "- audio\n",
      " bluebell\n",
      " alibi\n",
      " carbonated\n",
      " cloture\n",
      "\n",
      "*Topic 13\n",
      "- ancestry\n",
      " bleating\n",
      " burlesque\n",
      " bafflement\n",
      " arresting\n",
      "\n",
      "*Topic 14\n",
      "- chilean\n",
      " biked\n",
      " cardinal\n",
      " admission\n",
      " backer\n",
      "\n",
      "*Topic 15\n",
      "- amplia\n",
      " clarity\n",
      " commander\n",
      " arranger\n",
      " bayonet\n",
      "\n",
      "*Topic 16\n",
      "- autobiographical\n",
      " capstone\n",
      " bombardment\n",
      " bodybuilder\n",
      " barren\n",
      "\n",
      "*Topic 17\n",
      "- alouis\n",
      " booby\n",
      " chastened\n",
      " aground\n",
      " bequeath\n",
      "\n",
      "*Topic 18\n",
      "- childless\n",
      " commiserating\n",
      " collapses\n",
      " climax\n",
      " collapsed\n",
      "\n",
      "*Topic 19\n",
      "- clowning\n",
      " carmaker\n",
      " arrhythmia\n",
      " bleakly\n",
      " asteroid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the top 5 words for each topic (by probability):\n",
    "n = 5\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(doc_topic): <class 'numpy.ndarray'>\n",
      "shape: (100, 20)\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
